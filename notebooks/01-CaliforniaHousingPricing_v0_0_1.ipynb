{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/w4bo/teaching-bigdata/blob/main/notebooks/01-CaliforniaHousingPricing_v0_0_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLtjKHj97vTU"
   },
   "source": [
    "# California housing pricing\n",
    "\n",
    "This notebook runs on Google Colab. Colab provides a serverless Jupyter notebook environment for interactive development. (At the moment, 2022) Google Colab is free to use like other G Suite products.\n",
    "\n",
    "In this laboratory we will build a simple data pipeline to get acquainted with the \"main\" steps necessary to transform your data.\n",
    "\n",
    "The data contains information from the 1990 California census. It does provide an accessible introductory dataset for teaching people about the basics of machine learning. For simplcity, we will import the data from a csv file. Check also: https://www.kaggle.com/camnugent/california-housing-prices\n",
    "\n",
    "From https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/\n",
    "\n",
    "    This data has metrics such as the population, median income, median housing price, and so on for each block group in California.\n",
    "    Block groups are the smallest geographical unit for which the US Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people). We will just call them “districts” for short\n",
    "    The goal is to build a model to predict the median housing price in any district, given all the other metrics\n",
    "\n",
    "In case you need help with preprocessing with Pandas, check:\n",
    "\n",
    "    https://github.com/w4bo/2022-bbs-dm/blob/main/notebooks/01-PandasFundaments.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2LtNXSQ2Cudg",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "First of all, we need to setup the Python environment by installing and importing the necessary Python dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6rbuWt9G6O-u",
    "outputId": "416ee9e1-a4a0-489a-d606-1f688398b4b8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import prov\n",
    "\n",
    "print(pd.__version__)\n",
    "print(sk.__version__)\n",
    "print(np.__version__)\n",
    "print(sns.__version__)\n",
    "print(prov.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wMzMkSBOC8a5",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Data collection\n",
    "\n",
    "Import the dataset. In this case, there is no need for ETL/integration since the dataset is ready for elaboration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "NIoIXwyo8M7h",
    "outputId": "1c196a4c-0755-46b7-c9b8-d1f61e7547da"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://w4bo.github.io/AA2324-unibo-bigdataandcloudplatforms/housing.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqSwzJb1DH3F",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Data understanding\n",
    "\n",
    "Dataset description\n",
    "\n",
    "1. `longitude`: A measure of how far west a house is; a higher value is farther west\n",
    "2. `latitude`: A measure of how far north a house is; a higher value is farther north\n",
    "3. `housingMedianAge`: Median age of a house within a block; a lower number is a newer building\n",
    "4. `totalRooms`: Total number of rooms within a block\n",
    "5. `totalBedrooms`: Total number of bedrooms within a block\n",
    "6. `population`: Total number of people residing within a block\n",
    "7. `households`: Total number of households, a group of people residing within a home unit, for a block\n",
    "8. `medianIncome`: Median income for households within a block of houses (measured in tens of thousands of US Dollars)\n",
    "9. `medianHouseValue`: Median house value for households within a block (measured in US Dollars)\n",
    "10. `oceanProximity`: Location of the house w.r.t ocean/sea\n",
    "\n",
    "Let's take a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VgxApYX69xfM",
    "outputId": "66673b53-1d02-4f1c-e308-e8f57409942a",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "JHLY2U_X8eCH",
    "outputId": "f4da936c-20a4-4df0-8335-0e1f85adcc21",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PIEJz45c92j0",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Memory usage\n",
    "\n",
    "What if I change float64 to float32?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5CB8oUKS91KX",
    "outputId": "b1687fb1-c371-4c3a-ee5e-16f3758ba8a2"
   },
   "outputs": [],
   "source": [
    "dff = df.copy(deep=True) # copy the dataframe\n",
    "for x in df.columns: # iterate over the columns\n",
    "    if dff[x].dtype == 'float64': # if the column has type `float64`\n",
    "        dff[x] = dff[x].astype('float32') # ... change it to `float32`\n",
    "dff.info() # show some statistics on the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXoRiu4jDMIt",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are some missing values in the column `total_bedorooms` what can we do?\n",
    "\n",
    "Most Machine Learning algorithms cannot work with missing features. We have three options:\n",
    "\n",
    "Get rid of the corresponding districts (i.e., drop the rows)\n",
    "\n",
    "    df.dropna(subset=[\"total_bedrooms\"])\n",
    "\n",
    "Get rid of the whole attribute (i.e., drop the columns)\n",
    "\n",
    "    df.drop(\"total_bedrooms\", axis=1)\n",
    "\n",
    "Set the values to some value (zero, the mean, the median, etc.)\n",
    "\n",
    "    df[\"total_bedrooms\"].fillna(df[\"total_bedrooms\"].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkmSKRxF-GYN",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Non-numeric attributes\n",
    "\n",
    "`ocean_proximity` is a text attribute so we cannot compute its median. Some options:\n",
    "- Get rid of the whole attribute. (`df.drop(\"ocean_proximity\", axis=1`)\n",
    "- Change from categorical to ordinal (e.g., `NEAR BAY` = 0, `INLAND` = 1)\n",
    "    - Can foresee any problem in this?\n",
    "    - ML algorithms will assume that two nearby values are more similar than two distant values. This may be fine in some cases (e.g., for ordered categories such as “bad”, “average”, “good”, “excellent”), but it is obviously not the case for the ocean_proximity column (for example, categories 0 and 4 are clearly more similar than categories 0 and 1). \n",
    "- Change from categorical to one hot encoding\n",
    "    - To fix this issue, a common solution is to create one binary attribute per category: one attribute equal to 1 when the category is “<1H OCEAN” (and 0 otherwise), another attribute equal to 1 when the category is “INLAND” (and 0 otherwise), and so on. This is called one-hot encoding, because only one attribute will be equal to 1 (hot), while the others will be 0 (cold). The new attributes are sometimes called dummy attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VAj82Vws-mSp",
    "outputId": "2b536ee2-f753-4cc0-dd7d-75508c2be252",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df[\"ocean_proximity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "cDv2_GBG-mhq",
    "outputId": "86bc6268-a3d1-40f8-ab3f-c067dcc61e6b",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df[\"ocean_proximity\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6b3r2fE-qbJ",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Change from categorical to ordinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y10EU48h-mog",
    "outputId": "f849bc1d-1cdb-43c4-bcbf-6b0effb02634"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "y = ordinal_encoder.fit_transform(df[[\"ocean_proximity\"]])\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8axZ5Lh-sQ5",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "From categorical to one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "Nf1gLkpM-uuh",
    "outputId": "7786175a-8308-4497-ef4e-ceddf5c8566b"
   },
   "outputs": [],
   "source": [
    "y = pd.get_dummies(df[\"ocean_proximity\"], prefix='ocean_proximity')\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNzioS6F-w4w",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 879
    },
    "id": "yIl7LGkI-z7J",
    "outputId": "e2e75963-4847-4657-8505-7e122484a22e"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "df.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_sRRRIV-11w",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Open questions:\n",
    "\n",
    "- `median_income` should be in dollars. However, it has a strange range. Why? \"you are told that the data has been scaled and capped at 15 (actually 15.0001) for higher median incomes, and at 0.5 (actually 0.4999) for lower median incomes. The numbers represent roughly tens of thousands of dollars. The numbers represent roughly tens of thousands of dollars\"\n",
    "- `housing_median_age` and `median_house_value` are capped. As to `median_house_value`, this is a serious problem since it is your target attribute (your labels). Your Machine Learning algorithms may learn that prices never go beyond that limit. You need to check with your client team (the team that will use your system’s output) to see if this is a problem or not. If they tell you that they need precise predictions even beyond 500,000USD, then you have mainly two options: (a) collect proper labels for the districts whose labels were capped, (b) remove those districts from the training set.\"\n",
    "- These attributes have very different scales. Should we scale them?\n",
    "- Many histograms are tail heavy: they extend much farther to the right of the median than to the left. This may make it a bit harder for some Machine Learning algorithms to detect patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZHmiuOH-9qR",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Are there relationships between variables?\n",
    "\n",
    "Create a grid of Axes such that each numeric variable in data will by shared across the y-axes across a single row and the x-axes across a single column. The diagonal plots are treated differently: a univariate distribution plot is drawn to show the marginal distribution of the data in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "L9fEBIAL-8uA",
    "outputId": "3b114615-d9d1-4b69-dac9-2aee8e6e8b00"
   },
   "outputs": [],
   "source": [
    "tmp = df[[\"median_income\", \"housing_median_age\", \"median_house_value\", \"households\", \"population\", \"total_rooms\"]]\n",
    "# sns.pairplot(tmp.sample(n=1000, random_state=42), hue=\"median_house_value\", markers='+')\n",
    "sns.pairplot(tmp.sample(n=1000, random_state=42), markers='+')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGctGA91_Fg3",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Scaling attributes\n",
    "\n",
    "Attributes have very different scales. Should we scale them?\n",
    "\n",
    "- Normalization is good to use when you know that the distribution of your data does not follow a Gaussian distribution. This can be useful in algorithms that do not assume any distribution of the data like K-Nearest Neighbors and Neural Networks.\n",
    "- Standardization, on the other hand, can be helpful in cases where the data follows a Gaussian distribution. However, this does not have to be necessarily true. Unlike normalization, standardization does not have a bounding range. So, even if you have outliers in your data, they will not be affected by standardization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FTw2cvm_JcI",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Min-max normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "ebEHLs5n_RM3",
    "outputId": "26e2c5da-ffae-451e-eb0f-459e667db1c5"
   },
   "outputs": [],
   "source": [
    "num_df = df.drop(columns=['ocean_proximity', 'median_house_value'])\n",
    "normalized_df = (num_df - num_df.min()) / (num_df.max() - num_df.min())\n",
    "normalized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9NOrwC_p_SXW",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "orbzVbyn_TVO",
    "outputId": "dc311019-66ff-4d6c-9ac9-37727f241692"
   },
   "outputs": [],
   "source": [
    "num_df = df.drop(columns=['ocean_proximity', 'median_house_value'])\n",
    "normalized_df = (num_df - num_df.mean()) / num_df.std()\n",
    "normalized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ytYEaEv_Yr2",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "JuflsEzF8jCv"
   },
   "outputs": [],
   "source": [
    "# For now we simply drop \"ocean_proximity\"\n",
    "if \"ocean_proximity\" in df.columns:\n",
    "  df = df.drop(\"ocean_proximity\", axis=1)\n",
    "\n",
    "# Let's create some dataset variation\n",
    "# dataset1: drop the rows containing the null values and the columns `latitude` and `longitude` \n",
    "dataset_v1 = df.copy(deep=True).dropna().drop([\"longitude\", \"latitude\"], axis=1)\n",
    "\n",
    "# dataset2: impute missing values with the average number of bedrooms \n",
    "dataset_v2 = df.copy(deep=True)\n",
    "dataset_v2[\"total_bedrooms\"] = dataset_v2[\"total_bedrooms\"].fillna(dataset_v2[\"total_bedrooms\"].mean())\n",
    "\n",
    "# Create the list of datasets\n",
    "datasets = [(dataset_v1, \"datasetv1\"), (dataset_v2, \"datasetv2\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "uqVj0Myk9Fju",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's import some machine learning models (here we are not addressing hyper-parameter tuning)\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "dt = DecisionTreeRegressor(random_state=0) # initialize decision tree regressor model\n",
    "lr = LinearRegression() # initialize a linear regressor model\n",
    "\n",
    "# Create the list of algorithms\n",
    "ml_algorithms = [(lr, \"lr\"), (dt, \"dt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 387
    },
    "id": "vqy4T65c9pBr",
    "outputId": "6c26902b-1cc9-4af4-bfae-1057d9267ec6",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "instances = []\n",
    "i = 0\n",
    "\n",
    "# For each dataset version...\n",
    "for dataset, dataset_version in datasets:\n",
    "  # Get the feature matrix\n",
    "  X = dataset.drop(columns=[\"median_house_value\"]).to_numpy()\n",
    "  # Get the train label array\n",
    "  y = dataset[\"median_house_value\"].to_numpy() \n",
    "  \n",
    "  # For each machine learning algorithm...\n",
    "  for ml_algorithm, ml_algorithm_version in ml_algorithms:\n",
    "    # Run the machine learning algorithm on the given dataset\n",
    "    # Each run is a pipeline instance that we shoul compare against the others\n",
    "    instance = {}\n",
    "    instance[\"id\"] = i  # store the id of the instance \n",
    "    instance[\"dataset\"] = dataset_version  # store the version of the dataset\n",
    "    instance[\"algorithm\"] = ml_algorithm_version  # store the version of the ml algorithm\n",
    "    instance[\"score\"] = cross_val_score(ml_algorithm, X, y, cv=10).mean()  # store the performance of the pipeline instance\n",
    "    instances = instances + [instance]\n",
    "    i += 1\n",
    "\n",
    "# Collect the results\n",
    "result = pd.DataFrame.from_dict(instances, orient='columns')\n",
    "sns.catplot(x = \"dataset\", y = \"score\", hue = \"algorithm\", data = result, kind = \"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uIBtLp5PLnel",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And what about data provenance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "!apt update -y\n",
    "!apt install graphviz -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "id": "V5zvVzyBFEdm",
    "outputId": "2aacb8f6-a3dd-4535-9ffb-90ce053a8488"
   },
   "outputs": [],
   "source": [
    "from prov.model import ProvDocument\n",
    "from prov.dot import prov_to_dot\n",
    "from IPython.display import Image\n",
    "\n",
    "# Create a new provenance document\n",
    "d1 = ProvDocument()  # d1 is now an empty provenance document\n",
    "d1.add_namespace('unibo', 'https://www.unibo.it')  # add the namespace\n",
    "d1.add_namespace('sk', 'https://scikit-learn.org/stable/')  # add the namespace\n",
    "agent = d1.agent('unibo:mfrancia')  # add an agent\n",
    "\n",
    "# For each dataset version...\n",
    "for dataset, dataset_version in datasets:\n",
    "  original_dataset = d1.entity('unibo:' + dataset_version)  # register the dataset\n",
    "  d1.wasAttributedTo(original_dataset, agent)  # attribute the dataset to the agent who created it\n",
    "  # For each machine learning algorithm...\n",
    "  for ml_algorithm, ml_algorithm_version in ml_algorithms:\n",
    "    algo = d1.activity('sk:' + ml_algorithm_version)  # register the algorithm as a (processing) activity\n",
    "    processed_dataset = d1.entity('unibo:' + ml_algorithm_version + \"_\" + dataset_version, {'sk:cv-score': '...'})  # create an activity represented the processed dataset\n",
    "    d1.used(algo, original_dataset)  # the activity used the dataset as input\n",
    "    d1.wasGeneratedBy(processed_dataset, algo)  # the processed dataset has been created by the algorithm\n",
    "    d1.wasDerivedFrom(processed_dataset, original_dataset)  # the processed dataset has been derived from the original one\n",
    "\n",
    "# visualize the graph\n",
    "dot = prov_to_dot(d1)\n",
    "dot.write_png('prov.png')\n",
    "Image('prov.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSfWwAX9_hI2",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Hands on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_wjb1Go_qXl"
   },
   "source": [
    "##### Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "id": "muUUkx0P_mSQ",
    "outputId": "1c882211-ae4c-4b46-f12d-d40ec6e903dc"
   },
   "outputs": [],
   "source": [
    "num_df = df.copy(deep=True) # do not change this line\n",
    "\n",
    "# Filling in (i.e., impute) missing values with the median value \n",
    "num_df[\"total_bedrooms\"] = 1 # change `1` with the proper solution \n",
    "\n",
    "# Add a new column: population_per_household = population / households\n",
    "num_df[\"population_per_household\"] = 1 # change `1` with the proper solution \n",
    "\n",
    "# Add a new column: rooms_per_household = total_rooms / households\n",
    "num_df[\"rooms_per_household\"] = 1 # change `1` with the proper solution \n",
    "\n",
    "# Add a new column: bedrooms_per_room = total_bedrooms / total_rooms\n",
    "num_df[\"bedrooms_per_room\"] = 1 # change `1` with the proper solution \n",
    "\n",
    "# Apply standardization to all the numeric columns\n",
    "num_df = pd.DataFrame() # change `pd.DataFrame()` with the proper solution \n",
    "\n",
    "# One hot encode `ocean_proximity` since it is a categorical attribute \n",
    "cat_df = pd.DataFrame() # change `pd.DataFrame()` with the proper solution (hint: pd.get_dummies)\n",
    "\n",
    "clean_df = pd.concat([num_df, cat_df], axis=1) # do not change this line\n",
    "clean_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2lJyR1lr_uPE",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Machine learning\n",
    "\n",
    "Can you build a better model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "RzzwzxQP_02f"
   },
   "outputs": [],
   "source": [
    "# Try your sk-learn model here"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "authorship_tag": "ABX9TyNILlVuMwaE40TWXW+P1D90",
   "collapsed_sections": [],
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
