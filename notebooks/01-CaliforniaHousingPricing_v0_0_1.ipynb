{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/w4bo/teaching-bigdata/blob/main/notebooks/01-CaliforniaHousingPricing_v0_0_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLtjKHj97vTU"
   },
   "source": [
    "# California housing pricing\n",
    "\n",
    "This notebook runs on Google Colab. Colab provides a serverless Jupyter notebook environment for interactive development. (At the moment, 2022) Google Colab is free to use like other G Suite products.\n",
    "\n",
    "In this laboratory we will build a simple data pipeline to get acquainted with the \"main\" steps necessary to transform your data.\n",
    "\n",
    "The data contains information from the 1990 California census. It does provide an accessible introductory dataset for teaching people about the basics of machine learning. For simplcity, we will import the data from a csv file. Check also: https://www.kaggle.com/camnugent/california-housing-prices\n",
    "\n",
    "From https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/\n",
    "\n",
    "    This data has metrics such as the population, median income, median housing price, and so on for each block group in California.\n",
    "    Block groups are the smallest geographical unit for which the US Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people). We will just call them “districts” for short\n",
    "    The goal is to build a model to predict the median housing price in any district, given all the other metrics\n",
    "\n",
    "In case you need help with preprocessing with Pandas, check:\n",
    "\n",
    "    https://github.com/w4bo/2022-bbs-dm/blob/main/notebooks/01-PandasFundaments.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2LtNXSQ2Cudg",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "First of all, we need to setup the Python environment by installing and importing the necessary Python dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6rbuWt9G6O-u",
    "outputId": "416ee9e1-a4a0-489a-d606-1f688398b4b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: prov[dot] in /usr/local/lib/python3.10/site-packages (2.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in /usr/local/lib/python3.10/site-packages (from prov[dot]) (2.8.2)\n",
      "Requirement already satisfied: rdflib>=4.2.1 in /usr/local/lib/python3.10/site-packages (from prov[dot]) (7.0.0)\n",
      "Requirement already satisfied: lxml>=3.3.5 in /usr/local/lib/python3.10/site-packages (from prov[dot]) (4.9.3)\n",
      "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.10/site-packages (from prov[dot]) (3.2.1)\n",
      "Requirement already satisfied: pydot>=1.2.0 in /usr/local/lib/python3.10/site-packages (from prov[dot]) (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.10/site-packages (from pydot>=1.2.0->prov[dot]) (3.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.2->prov[dot]) (1.16.0)\n",
      "Requirement already satisfied: isodate<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/site-packages (from rdflib>=4.2.1->prov[dot]) (0.6.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "2.1.2\n",
      "1.3.0\n",
      "1.26.0\n",
      "0.12.2\n",
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import prov\n",
    "\n",
    "print(pd.__version__)\n",
    "print(sk.__version__)\n",
    "print(np.__version__)\n",
    "print(sns.__version__)\n",
    "print(prov.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wMzMkSBOC8a5",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Data collection\n",
    "\n",
    "Import the dataset. In this case, there is no need for ETL/integration since the dataset is ready for elaboration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "NIoIXwyo8M7h",
    "outputId": "1c196a4c-0755-46b7-c9b8-d1f61e7547da"
   },
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://raw.githubusercontent.com/w4bo/handsOnDataPipelines/main/01-MachineLearning/datasets/housing.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m df\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/io/common.py:718\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    715\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[1;32m    717\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[0;32m--> 718\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    726\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[1;32m    727\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/io/common.py:372\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[1;32m    371\u001b[0m req_info \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(filepath_or_buffer, headers\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[0;32m--> 372\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq_info\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[1;32m    373\u001b[0m     content_encoding \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    375\u001b[0m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/io/common.py:274\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03mthe stdlib.\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/urllib/request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[1;32m    524\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/urllib/request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m--> 634\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/urllib/request.py:563\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[1;32m    562\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/urllib/request.py:643\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[0;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/w4bo/handsOnDataPipelines/main/01-MachineLearning/datasets/housing.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqSwzJb1DH3F",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Data understanding\n",
    "\n",
    "Dataset description\n",
    "\n",
    "1. `longitude`: A measure of how far west a house is; a higher value is farther west\n",
    "2. `latitude`: A measure of how far north a house is; a higher value is farther north\n",
    "3. `housingMedianAge`: Median age of a house within a block; a lower number is a newer building\n",
    "4. `totalRooms`: Total number of rooms within a block\n",
    "5. `totalBedrooms`: Total number of bedrooms within a block\n",
    "6. `population`: Total number of people residing within a block\n",
    "7. `households`: Total number of households, a group of people residing within a home unit, for a block\n",
    "8. `medianIncome`: Median income for households within a block of houses (measured in tens of thousands of US Dollars)\n",
    "9. `medianHouseValue`: Median house value for households within a block (measured in US Dollars)\n",
    "10. `oceanProximity`: Location of the house w.r.t ocean/sea\n",
    "\n",
    "Let's take a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VgxApYX69xfM",
    "outputId": "66673b53-1d02-4f1c-e308-e8f57409942a",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "JHLY2U_X8eCH",
    "outputId": "f4da936c-20a4-4df0-8335-0e1f85adcc21",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PIEJz45c92j0",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Memory usage\n",
    "\n",
    "What if I change float64 to float32?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5CB8oUKS91KX",
    "outputId": "b1687fb1-c371-4c3a-ee5e-16f3758ba8a2"
   },
   "outputs": [],
   "source": [
    "dff = df.copy(deep=True) # copy the dataframe\n",
    "for x in df.columns: # iterate over the columns\n",
    "    if dff[x].dtype == 'float64': # if the column has type `float64`\n",
    "        dff[x] = dff[x].astype('float32') # ... change it to `float32`\n",
    "dff.info() # show some statistics on the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXoRiu4jDMIt",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are some missing values in the column `total_bedorooms` what can we do?\n",
    "\n",
    "Most Machine Learning algorithms cannot work with missing features. We have three options:\n",
    "\n",
    "Get rid of the corresponding districts (i.e., drop the rows)\n",
    "\n",
    "    df.dropna(subset=[\"total_bedrooms\"])\n",
    "\n",
    "Get rid of the whole attribute (i.e., drop the columns)\n",
    "\n",
    "    df.drop(\"total_bedrooms\", axis=1)\n",
    "\n",
    "Set the values to some value (zero, the mean, the median, etc.)\n",
    "\n",
    "    df[\"total_bedrooms\"].fillna(df[\"total_bedrooms\"].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkmSKRxF-GYN",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Non-numeric attributes\n",
    "\n",
    "`ocean_proximity` is a text attribute so we cannot compute its median. Some options:\n",
    "- Get rid of the whole attribute. (`df.drop(\"ocean_proximity\", axis=1`)\n",
    "- Change from categorical to ordinal (e.g., `NEAR BAY` = 0, `INLAND` = 1)\n",
    "    - Can foresee any problem in this?\n",
    "    - ML algorithms will assume that two nearby values are more similar than two distant values. This may be fine in some cases (e.g., for ordered categories such as “bad”, “average”, “good”, “excellent”), but it is obviously not the case for the ocean_proximity column (for example, categories 0 and 4 are clearly more similar than categories 0 and 1). \n",
    "- Change from categorical to one hot encoding\n",
    "    - To fix this issue, a common solution is to create one binary attribute per category: one attribute equal to 1 when the category is “<1H OCEAN” (and 0 otherwise), another attribute equal to 1 when the category is “INLAND” (and 0 otherwise), and so on. This is called one-hot encoding, because only one attribute will be equal to 1 (hot), while the others will be 0 (cold). The new attributes are sometimes called dummy attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VAj82Vws-mSp",
    "outputId": "2b536ee2-f753-4cc0-dd7d-75508c2be252",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df[\"ocean_proximity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "cDv2_GBG-mhq",
    "outputId": "86bc6268-a3d1-40f8-ab3f-c067dcc61e6b",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df[\"ocean_proximity\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6b3r2fE-qbJ",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Change from categorical to ordinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y10EU48h-mog",
    "outputId": "f849bc1d-1cdb-43c4-bcbf-6b0effb02634"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "y = ordinal_encoder.fit_transform(df[[\"ocean_proximity\"]])\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8axZ5Lh-sQ5",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "From categorical to one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "Nf1gLkpM-uuh",
    "outputId": "7786175a-8308-4497-ef4e-ceddf5c8566b"
   },
   "outputs": [],
   "source": [
    "y = pd.get_dummies(df[\"ocean_proximity\"], prefix='ocean_proximity')\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNzioS6F-w4w",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 879
    },
    "id": "yIl7LGkI-z7J",
    "outputId": "e2e75963-4847-4657-8505-7e122484a22e"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "df.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_sRRRIV-11w",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Open questions:\n",
    "\n",
    "- `median_income` should be in dollars. However, it has a strange range. Why? \"you are told that the data has been scaled and capped at 15 (actually 15.0001) for higher median incomes, and at 0.5 (actually 0.4999) for lower median incomes. The numbers represent roughly tens of thousands of dollars. The numbers represent roughly tens of thousands of dollars\"\n",
    "- `housing_median_age` and `median_house_value` are capped. As to `median_house_value`, this is a serious problem since it is your target attribute (your labels). Your Machine Learning algorithms may learn that prices never go beyond that limit. You need to check with your client team (the team that will use your system’s output) to see if this is a problem or not. If they tell you that they need precise predictions even beyond 500,000USD, then you have mainly two options: (a) collect proper labels for the districts whose labels were capped, (b) remove those districts from the training set.\"\n",
    "- These attributes have very different scales. Should we scale them?\n",
    "- Many histograms are tail heavy: they extend much farther to the right of the median than to the left. This may make it a bit harder for some Machine Learning algorithms to detect patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZHmiuOH-9qR",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Are the relationships between variables?\n",
    "\n",
    "Create a grid of Axes such that each numeric variable in data will by shared across the y-axes across a single row and the x-axes across a single column. The diagonal plots are treated differently: a univariate distribution plot is drawn to show the marginal distribution of the data in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "L9fEBIAL-8uA",
    "outputId": "3b114615-d9d1-4b69-dac9-2aee8e6e8b00"
   },
   "outputs": [],
   "source": [
    "tmp = df[[\"median_income\", \"housing_median_age\", \"median_house_value\", \"households\", \"population\", \"total_rooms\"]]\n",
    "# sns.pairplot(tmp.sample(n=1000, random_state=42), hue=\"median_house_value\", markers='+')\n",
    "sns.pairplot(tmp.sample(n=1000, random_state=42), markers='+')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGctGA91_Fg3",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Scaling attributes\n",
    "\n",
    "Attributes have very different scales. Should we scale them?\n",
    "\n",
    "- Normalization is good to use when you know that the distribution of your data does not follow a Gaussian distribution. This can be useful in algorithms that do not assume any distribution of the data like K-Nearest Neighbors and Neural Networks.\n",
    "- Standardization, on the other hand, can be helpful in cases where the data follows a Gaussian distribution. However, this does not have to be necessarily true. Unlike normalization, standardization does not have a bounding range. So, even if you have outliers in your data, they will not be affected by standardization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FTw2cvm_JcI",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Min-max normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "ebEHLs5n_RM3",
    "outputId": "26e2c5da-ffae-451e-eb0f-459e667db1c5"
   },
   "outputs": [],
   "source": [
    "num_df = df.drop(columns=['ocean_proximity', 'median_house_value'])\n",
    "normalized_df = (num_df - num_df.min()) / (num_df.max() - num_df.min())\n",
    "normalized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9NOrwC_p_SXW",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "orbzVbyn_TVO",
    "outputId": "dc311019-66ff-4d6c-9ac9-37727f241692"
   },
   "outputs": [],
   "source": [
    "num_df = df.drop(columns=['ocean_proximity', 'median_house_value'])\n",
    "normalized_df = (num_df - num_df.mean()) / num_df.std()\n",
    "normalized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ytYEaEv_Yr2",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JuflsEzF8jCv"
   },
   "outputs": [],
   "source": [
    "# For now we simply drop \"ocean_proximity\"\n",
    "if \"ocean_proximity\" in df.columns:\n",
    "  df = df.drop(\"ocean_proximity\", axis=1)\n",
    "\n",
    "# Let's create some dataset variation\n",
    "# dataset1: drop the rows containing the null values and the columns `latitude` and `longitude` \n",
    "dataset_v1 = df.copy(deep=True).dropna().drop([\"longitude\", \"latitude\"], axis=1)\n",
    "\n",
    "# dataset2: impute missing values with the average number of bedrooms \n",
    "dataset_v2 = df.copy(deep=True)\n",
    "dataset_v2[\"total_bedrooms\"] = dataset_v2[\"total_bedrooms\"].fillna(dataset_v2[\"total_bedrooms\"].mean())\n",
    "\n",
    "# Create the list of datasets\n",
    "datasets = [(dataset_v1, \"datasetv1\"), (dataset_v2, \"datasetv2\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uqVj0Myk9Fju",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's import some machine learning models (here we are not addressing hyper-parameter tuning)\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "dt = DecisionTreeRegressor(random_state=0) # initialize decision tree regressor model\n",
    "lr = LinearRegression() # initialize a linear regressor model\n",
    "\n",
    "# Create the list of algorithms\n",
    "ml_algorithms = [(lr, \"lr\"), (dt, \"dt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 387
    },
    "id": "vqy4T65c9pBr",
    "outputId": "6c26902b-1cc9-4af4-bfae-1057d9267ec6",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "instances = []\n",
    "i = 0\n",
    "\n",
    "# For each dataset version...\n",
    "for dataset, dataset_version in datasets:\n",
    "  # Get the feature matrix\n",
    "  X = dataset.drop(columns=[\"median_house_value\"]).to_numpy()\n",
    "  # Get the train label array\n",
    "  y = dataset[\"median_house_value\"].to_numpy() \n",
    "  \n",
    "  # For each machine learning algorithm...\n",
    "  for ml_algorithm, ml_algorithm_version in ml_algorithms:\n",
    "    # Run the machine learning algorithm on the given dataset\n",
    "    # Each run is a pipeline instance that we shoul compare against the others\n",
    "    instance = {}\n",
    "    instance[\"id\"] = i  # store the id of the instance \n",
    "    instance[\"dataset\"] = dataset_version  # store the version of the dataset\n",
    "    instance[\"algorithm\"] = ml_algorithm_version  # store the version of the ml algorithm\n",
    "    instance[\"score\"] = cross_val_score(ml_algorithm, X, y, cv=10).mean()  # store the performance of the pipeline instance\n",
    "    instances = instances + [instance]\n",
    "    i += 1\n",
    "\n",
    "# Collect the results\n",
    "result = pd.DataFrame.from_dict(instances, orient='columns')\n",
    "sns.catplot(x = \"dataset\", y = \"score\", hue = \"algorithm\", data = result, kind = \"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uIBtLp5PLnel",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And what about data provenance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "id": "V5zvVzyBFEdm",
    "outputId": "2aacb8f6-a3dd-4535-9ffb-90ce053a8488"
   },
   "outputs": [],
   "source": [
    "from prov.model import ProvDocument\n",
    "from prov.dot import prov_to_dot\n",
    "from IPython.display import Image\n",
    "\n",
    "# Create a new provenance document\n",
    "d1 = ProvDocument()  # d1 is now an empty provenance document\n",
    "d1.add_namespace('unibo', 'https://www.unibo.it')  # add the namespace\n",
    "d1.add_namespace('sk', 'https://scikit-learn.org/stable/')  # add the namespace\n",
    "agent = d1.agent('unibo:mfrancia')  # add an agent\n",
    "\n",
    "# For each dataset version...\n",
    "for dataset, dataset_version in datasets:\n",
    "  original_dataset = d1.entity('unibo:' + dataset_version)  # register the dataset\n",
    "  d1.wasAttributedTo(original_dataset, agent)  # attribute the dataset to the agent who created it\n",
    "  # For each machine learning algorithm...\n",
    "  for ml_algorithm, ml_algorithm_version in ml_algorithms:\n",
    "    algo = d1.activity('sk:' + ml_algorithm_version)  # register the algorithm as a (processing) activity\n",
    "    processed_dataset = d1.entity('unibo:' + ml_algorithm_version + \"_\" + dataset_version, {'sk:cv-score': '...'})  # create an activity represented the processed dataset\n",
    "    d1.used(algo, original_dataset)  # the activity used the dataset as input\n",
    "    d1.wasGeneratedBy(processed_dataset, algo)  # the processed dataset has been created by the algorithm\n",
    "    d1.wasDerivedFrom(processed_dataset, original_dataset)  # the processed dataset has been derived from the original one\n",
    "\n",
    "# visualize the graph\n",
    "dot = prov_to_dot(d1)\n",
    "dot.write_png('prov.png')\n",
    "Image('prov.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSfWwAX9_hI2",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Hands on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_wjb1Go_qXl"
   },
   "source": [
    "##### Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "id": "muUUkx0P_mSQ",
    "outputId": "1c882211-ae4c-4b46-f12d-d40ec6e903dc"
   },
   "outputs": [],
   "source": [
    "num_df = df.copy(deep=True) # do not change this line\n",
    "\n",
    "# Filling in (i.e., impute) missing values with the median value \n",
    "num_df[\"total_bedrooms\"] = 1 # change `1` with the proper solution \n",
    "\n",
    "# Add a new column: population_per_household = population / households\n",
    "num_df[\"population_per_household\"] = 1 # change `1` with the proper solution \n",
    "\n",
    "# Add a new column: rooms_per_household = total_rooms / households\n",
    "num_df[\"rooms_per_household\"] = 1 # change `1` with the proper solution \n",
    "\n",
    "# Add a new column: bedrooms_per_room = total_bedrooms / total_rooms\n",
    "num_df[\"bedrooms_per_room\"] = 1 # change `1` with the proper solution \n",
    "\n",
    "# Apply standardization to all the numeric columns\n",
    "num_df = pd.DataFrame() # change `pd.DataFrame()` with the proper solution \n",
    "\n",
    "# One hot encode `ocean_proximity` since it is a categorical attribute \n",
    "cat_df = pd.DataFrame() # change `pd.DataFrame()` with the proper solution (hint: pd.get_dummies)\n",
    "\n",
    "clean_df = pd.concat([num_df, cat_df], axis=1) # do not change this line\n",
    "clean_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2lJyR1lr_uPE",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Machine learning\n",
    "\n",
    "Can you build a better model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RzzwzxQP_02f"
   },
   "outputs": [],
   "source": [
    "# Try your sk-learn model here"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "authorship_tag": "ABX9TyNILlVuMwaE40TWXW+P1D90",
   "collapsed_sections": [],
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
